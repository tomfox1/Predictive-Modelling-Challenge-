{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Wrangling_Ideas.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomfox1/Predictive-Modelling-Challenge-/blob/master/Data_Wrangling_Ideas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySkD3JhntUOl",
        "colab_type": "text"
      },
      "source": [
        "# Add your ideas for data wrangling in a cell below. Add code if you would like or just ideas for features/ways to clean the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi9NIF7UtxMx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   Population around each well, higher pop-density would mean higher wear\n",
        "2.   Does elevation interact with condition of well\n",
        "3.   Airpollution? compare polution in the area to the longevity of the well \n",
        "4.   Does a certain installer have longer lasting wells? \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbFYOCm6KK_8",
        "colab_type": "text"
      },
      "source": [
        "let's get a majority <-?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1S2sEZCuRSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###!!! DONT JUST COPY AND PASTE\n",
        "\n",
        "def MrClean(df):\n",
        "    df_t= df\n",
        "    df_t['gps_height'].replace(0.0, np.nan, inplace=True)\n",
        "    df_t['population'].replace(0.0, np.nan, inplace=True)\n",
        "    df_t['amount_tsh'].replace(0.0, np.nan, inplace=True)\n",
        "    df_t['gps_height'].fillna(df_t.groupby(['region', 'district_code'])['gps_height'].transform('mean'), inplace=True)\n",
        "    df_t['gps_height'].fillna(df_t.groupby(['region'])['gps_height'].transform('mean'), inplace=True)\n",
        "    df_t['gps_height'].fillna(df_t['gps_height'].mean(), inplace=True)\n",
        "    df_t['population'].fillna(df_t.groupby(['region', 'district_code'])['population'].transform('median'), inplace=True)\n",
        "    df_t['population'].fillna(df_t.groupby(['region'])['population'].transform('median'), inplace=True)\n",
        "    df_t['population'].fillna(df_t['population'].median(), inplace=True)\n",
        "    df_t['amount_tsh'].fillna(df_t.groupby(['region', 'district_code'])['amount_tsh'].transform('median'), inplace=True)\n",
        "    df_t['amount_tsh'].fillna(df_t.groupby(['region'])['amount_tsh'].transform('median'), inplace=True)\n",
        "    df_t['amount_tsh'].fillna(df_t['amount_tsh'].median(), inplace=True)\n",
        "    features=['amount_tsh', 'gps_height', 'population']\n",
        "    scaler = MinMaxScaler(feature_range=(0,20))\n",
        "    df_t[features] = scaler.fit_transform(df_t[features])\n",
        "    df_t['longitude'].replace(0.0, np.nan, inplace=True)\n",
        "    df_t['latitude'].replace(0.0, np.nan, inplace=True)\n",
        "    df_t['construction_year'].replace(0.0, np.nan, inplace=True)\n",
        "    df_t['latitude'].fillna(df_t.groupby(['region', 'district_code'])['latitude'].transform('mean'), inplace=True)\n",
        "    df_t['longitude'].fillna(df_t.groupby(['region', 'district_code'])['longitude'].transform('mean'), inplace=True)\n",
        "    df_t['longitude'].fillna(df_t.groupby(['region'])['longitude'].transform('mean'), inplace=True)\n",
        "    df_t['construction_year'].fillna(df_t.groupby(['region', 'district_code'])['construction_year'].transform('median'), inplace=True)\n",
        "    df_t['construction_year'].fillna(df_t.groupby(['region'])['construction_year'].transform('median'), inplace=True)\n",
        "    df_t['construction_year'].fillna(df_t.groupby(['district_code'])['construction_year'].transform('median'), inplace=True)\n",
        "    df_t['construction_year'].fillna(df_t['construction_year'].median(), inplace=True)\n",
        "    df_t['date_recorded'] = pd.to_datetime(df_t['date_recorded'])\n",
        "    df_t['years_service'] = df_t.date_recorded.dt.year - df_t.construction_year\n",
        "   \n",
        "    garbage=['wpt_name','num_private','subvillage','region_code','recorded_by','management_group',\n",
        "         'extraction_type_group','extraction_type_class','scheme_name','payment',\n",
        "        'quality_group','quantity_group','source_type','source_class','waterpoint_type_group',\n",
        "        'ward','public_meeting','permit','date_recorded','construction_year']\n",
        "    df_t.drop(garbage,axis=1, inplace=True)\n",
        "    df_t.waterpoint_type = df_t.waterpoint_type.str.lower()\n",
        "    df_t.funder = df_t.funder.str.lower()\n",
        "    df_t.basin = df_t.basin.str.lower()\n",
        "    df_t.region = df_t.region.str.lower()\n",
        "    df_t.source = df_t.source.str.lower()\n",
        "    df_t.lga = df_t.lga.str.lower()\n",
        "    df_t.management = df_t.management.str.lower()\n",
        "    df_t.quantity = df_t.quantity.str.lower()\n",
        "    df_t.water_quality = df_t.water_quality.str.lower()\n",
        "    df_t.payment_type=df_t.payment_type.str.lower()\n",
        "    df_t.extraction_type=df_t.extraction_type.str.lower()\n",
        "    df_t[\"funder\"].fillna(\"other\", inplace=True)\n",
        "    df_t[\"scheme_management\"].fillna(\"other\", inplace=True)\n",
        "    df_t[\"installer\"].fillna(\"other\", inplace=True)\n",
        "    \n",
        "    #further spacial/location information\n",
        "    #https://www.kaggle.com/c/sf-crime/discussion/18853\n",
        "    \n",
        "    return df_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6ekXkwsL8To",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def con_matrix_analysis(model, feature_matrix, target_vector):\n",
        "  from sklearn.metrics import classification_report, confusion_matrix\n",
        "  \n",
        "  x = model.predict(feature_matrix)\n",
        "  y = target_vector\n",
        "  \n",
        "  print(classification_report(y, x,\n",
        "        target_names=['Functional', 'Needs Repair', 'Not-Functional']))\n",
        "\n",
        "  con_matrix = pd.DataFrame(confusion_matrix(y, x), \n",
        "             columns=['Predicted Functional', 'Predicted Needs Repair', 'Predicted Not-Functional'], \n",
        "             index=['Actual Functional', 'Actual Needs Repair', 'Actual Not-Functional'])\n",
        "                            \n",
        "  sns.heatmap(data=con_matrix, cmap='cool')\n",
        "  plt.show();\n",
        "  return con_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1S61wznQ_TB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reverse_cardinality_check(n, df):\n",
        "  \"\"\"\n",
        "  Given a cardinality limit (n) and a dataframe this function will search the\n",
        "  dataframe for features above the cardinality limit, then create a dict\n",
        "  from the results\n",
        "  \"\"\"\n",
        "  \n",
        "  feature_list = []\n",
        "  \n",
        "  cardinality_value = []\n",
        "  \n",
        "  for _ in range(len(df.columns)):\n",
        "    if len(df[df.columns[_]].value_counts()) > n:\n",
        "      \n",
        "      feature_list.append(df.columns[_])\n",
        "      \n",
        "      cardinality_value.append(len(df[df.columns[_]].value_counts()))\n",
        "                               \n",
        "        \n",
        "  feature_dict = dict(zip(feature_list, cardinality_value))\n",
        "  \n",
        "  return feature_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZMNUGDw5awV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wrangle(X):\n",
        "    # Create copy of dataframe to avoid copy warning\n",
        "    X = X.copy()\n",
        "    \n",
        "    # Some features have missing data showing as 0 that need to be changed to nan\n",
        "    X['gps_height'] = X['gps_height'].replace(0.0, np.nan)\n",
        "    X['longitude'] = X['longitude'].replace(0.0, np.nan)\n",
        "    X['latitude'] = X['latitude'].replace(0.0, np.nan)\n",
        "    X['construction_year'] = X['construction_year'].replace(0.0, np.nan)\n",
        "    X['population'] = X['population'].replace(0.0, np.nan)\n",
        "    X['amount_tsh'] = X['amount_tsh'].replace(0.0, np.nan)\n",
        "    \n",
        "    # gps_height: replace nan values with the mean of the smallest geographical region possible\n",
        "    # Excluding subvillage due to missing values\n",
        "    X['gps_height'].fillna(X.groupby(['ward'])['gps_height'].transform('mean'), inplace=True)\n",
        "    X['gps_height'].fillna(X.groupby(['district_code'])['gps_height'].transform('mean'), inplace=True)\n",
        "    X['gps_height'].fillna(X.groupby(['region_code'])['gps_height'].transform('mean'), inplace=True)\n",
        "    X['gps_height'].fillna(X['gps_height'].mean(), inplace=True)\n",
        "    \n",
        "    # longitude: replace nan values with the mean of the smallest geographical region possible\n",
        "    # Excluding subvillage due to missing values\n",
        "    X['longitude'].fillna(X.groupby(['ward'])['longitude'].transform('mean'), inplace=True)\n",
        "    X['longitude'].fillna(X.groupby(['district_code'])['longitude'].transform('mean'), inplace=True)\n",
        "    X['longitude'].fillna(X.groupby(['region_code'])['longitude'].transform('mean'), inplace=True)\n",
        "    X['longitude'].fillna(X['longitude'].mean(), inplace=True)\n",
        "    \n",
        "    # latitude: replace nan values with the mean of the smallest geographical region possible\n",
        "    # Excluding subvillage due to missing values\n",
        "    X['latitude'].fillna(X.groupby(['ward'])['latitude'].transform('mean'), inplace=True)\n",
        "    X['latitude'].fillna(X.groupby(['district_code'])['latitude'].transform('mean'), inplace=True)\n",
        "    X['latitude'].fillna(X.groupby(['region_code'])['latitude'].transform('mean'), inplace=True)\n",
        "    X['latitude'].fillna(X['latitude'].mean(), inplace=True)\n",
        "    \n",
        "    # population: replace nan values with the mean of the smallest geographical region possible\n",
        "    # Excluding subvillage due to missing values\n",
        "    X['population'].fillna(X.groupby(['ward'])['population'].transform('median'), inplace=True)\n",
        "    X['population'].fillna(X.groupby(['district_code'])['population'].transform('median'), inplace=True)\n",
        "    X['population'].fillna(X.groupby(['region_code'])['population'].transform('median'), inplace=True)\n",
        "    X['population'].fillna(X['population'].median(), inplace=True)\n",
        "    \n",
        "    # population: replace nan values with the mean of the smallest geographical region possible\n",
        "    # Excluding subvillage due to missing values\n",
        "    X['amount_tsh'].fillna(X.groupby(['ward'])['amount_tsh'].transform('median'), inplace=True)\n",
        "    X['amount_tsh'].fillna(X.groupby(['district_code'])['amount_tsh'].transform('median'), inplace=True)\n",
        "    X['amount_tsh'].fillna(X.groupby(['region_code'])['amount_tsh'].transform('median'), inplace=True)\n",
        "    X['amount_tsh'].fillna(X['amount_tsh'].median(), inplace=True)\n",
        "    \n",
        "    # construction_year: replace nan values with the mean of the smallest geographical region possible\n",
        "    # Excluding subvillage due to missing values\n",
        "    X['construction_year'].fillna(X.groupby(['ward'])['construction_year'].transform('median'), inplace=True)\n",
        "    X['construction_year'].fillna(X.groupby(['district_code'])['construction_year'].transform('median'), inplace=True)\n",
        "    X['construction_year'].fillna(X.groupby(['region_code'])['construction_year'].transform('median'), inplace=True)\n",
        "    X['construction_year'].fillna(X['construction_year'].median(), inplace=True)\n",
        "    \n",
        "    # Convert date to datetime\n",
        "    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
        "    \n",
        "    # Extract datetime data\n",
        "    X['year_recorded'] = X['date_recorded'].dt.year\n",
        "    \n",
        "    # Drop duplicate or unnecessary features\n",
        "    X = X.drop(columns=['recorded_by', 'quantity_group', 'date_recorded', 'wpt_name', 'num_private', 'subvillage',\n",
        "                       'region_code', 'management_group', 'extraction_type_group', 'extraction_type_class',\n",
        "                       'scheme_name', 'payment', 'water_quality', 'source_type', 'source_class', 'waterpoint_type_group',\n",
        "                       'ward', 'public_meeting', 'permit'])\n",
        "    \n",
        "    # Several categorical features have values showing as '0'\n",
        "    # Replace '0' with nan\n",
        "    categoricals = X.select_dtypes(exclude='number').columns.tolist()\n",
        "    X[categoricals] = X[categoricals].replace('0', np.nan)\n",
        "    \n",
        "    # Convert to lowercase to collapse duplicates\n",
        "    X['waterpoint_type'] = X['waterpoint_type'].str.lower()\n",
        "    X['funder'] = X['funder'].str.lower()\n",
        "    X['basin'] = X['basin'].str.lower()\n",
        "    X['region'] = X['region'].str.lower()\n",
        "    X['source'] = X['source'].str.lower()\n",
        "    X['lga'] = X['lga'].str.lower()\n",
        "    X['management'] = X['management'].str.lower()\n",
        "    X['quantity'] = X['quantity'].str.lower()\n",
        "    X['quality_group'] = X['quality_group'].str.lower()\n",
        "    X['payment_type'] = X['payment_type'].str.lower()\n",
        "    X['extraction_type'] = X['extraction_type'].str.lower()\n",
        "    \n",
        "    # Replace nan values with 'other'\n",
        "    X[\"funder\"].fillna(\"other\", inplace=True)\n",
        "    X[\"scheme_management\"].fillna(\"other\", inplace=True)\n",
        "    X[\"installer\"].fillna(\"other\", inplace=True)\n",
        "    \n",
        "    # Replace any remaining nan values with 'other'\n",
        "    X = X.replace(np.nan, 'other')\n",
        "    \n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGcRO7nXiZzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These functions do an excellent job cleaning up the data and also do some conservative feature engineering\n",
        "\n",
        "def cleanup1(X):\n",
        "  X2 = X.copy()\n",
        "\n",
        "  \n",
        "  X2 = X2.fillna('unknown')\n",
        "  dates = pd.to_datetime(X2.date_recorded)\n",
        "  year2000 = pd.to_datetime('2000-01-01')\n",
        "  years = [i.days/365 for i in (dates - year2000)]\n",
        "  X2.date_recorded = years\n",
        "\n",
        "  X2.region_code = X2.region_code.astype('str')\n",
        "  X2.district_code = X2.district_code.astype('str')\n",
        "\n",
        "  X2 = X2.drop(columns='recorded_by')\n",
        "\n",
        "  type_dict = {'amount_tsh':'float64','date_recorded':'float64',\n",
        "                 'gps_height':'float64',\n",
        "                 'longitude':'float64',\n",
        "                 'latitude':'float64',\n",
        "                 'num_private':'float64',\n",
        "                 'population':'float64',\n",
        "                 'construction_year':'float64',\n",
        "                 'public_meeting':'str',\n",
        "                 'permit':'str'}\n",
        "    \n",
        "  X2 = X2.astype(dtype = type_dict)\n",
        "    \n",
        "  return X2\n",
        "\n",
        "from sklearn.impute import MissingIndicator\n",
        "\n",
        "def cleanup2(X):\n",
        "  X2 = X.copy()\n",
        "    \n",
        "    # I make a list of the numerical columns and a dict of their \n",
        "    # garbage values that really should be nulls\n",
        "  numericals = ['amount_tsh',\n",
        "                    'date_recorded',\n",
        "                    'gps_height',\n",
        "                    'longitude',\n",
        "                    'latitude',\n",
        "                    'num_private',\n",
        "                    'population',\n",
        "                    'construction_year']\n",
        "\n",
        "  null_values = {'amount_tsh':0,\n",
        "                     'date_recorded':0,\n",
        "                     'gps_height':0,\n",
        "                     'longitude':0,\n",
        "                     'latitude':-2.000000e-08,\n",
        "                     'num_private':0,\n",
        "                     'population':0,\n",
        "                     'construction_year':0}\n",
        "\n",
        "    # I replace all garbage values with NANs.\n",
        "  for feature, null in null_values.items():\n",
        "        X2[feature] = X2[feature].replace(null, np.nan)\n",
        "\n",
        "    # construction_year occasionally claims years far in the future, and \n",
        "    # could presumably also contain years way in the past.  I'll turn anything\n",
        "    # not between 1960 and 2019 into a NAN.\n",
        "  X2['construction_year'] = [i if 1960 < i < 2019 else np.nan for i in X2['construction_year']]\n",
        "  indicator = MissingIndicator()\n",
        "  trash_array = indicator.fit_transform(X2[numericals]) # Bool array\n",
        "  trash_array = trash_array.astype('float64')     # Float64 array\n",
        "\n",
        "    # Create a titles for the columns in num_trashmarker\n",
        "  trashy_names = [numericals[i] + '_trash' for i in indicator.features_]\n",
        "\n",
        "    # Create num_trashmarker\n",
        "  trash_df = pd.DataFrame(trash_array, columns=trashy_names)\n",
        "\n",
        "    # I add trash_df to X2\n",
        "  X2 = pd.concat([X2,trash_df], sort=False, axis=1)\n",
        "    \n",
        "    \n",
        "    # Fixing the numerical columns.\n",
        "    # ---------------------------------------------------------------\n",
        "    # Whenever possible, a good replacement value for a NAN is the \n",
        "    # mean or median value for the geographic region around it.\n",
        "\n",
        "    # Replaces the NANs in a ward with the mean of the other rows in that \n",
        "    # same ward. If all the rows in a ward are NANs, though, they remain.\n",
        "  for feature in numericals:\n",
        "      replacements = X2.groupby('ward')[feature].transform('mean')\n",
        "      X2[feature] = X2[feature].fillna(replacements)\n",
        "\n",
        "    # Replaces the NANs in a region with the mean of the other rows in that \n",
        "    # same region (which are much larger than wards)\n",
        "  for feature in numericals:\n",
        "      replacements = X2.groupby('region')[feature].transform('mean')\n",
        "      X2[feature] = X2[feature].fillna(replacements)\n",
        "    \n",
        "    # Replaces any remaining NANs with the median value for the whole dataset\n",
        "  for feature in numericals:\n",
        "      replacements = X2[feature].median() # Single number, not array\n",
        "      X2[feature] = X2[feature].fillna(replacements)\n",
        "    \n",
        "  return X2\n",
        "\n",
        "def cleanup3(X):\n",
        "  X2 = X.copy()\n",
        "    \n",
        "    # Create list of categorical features\n",
        "  categoricals = X2.select_dtypes(exclude='number').columns.tolist()\n",
        "\n",
        "    # Make all strings lowercase, to collapse together some of the categories\n",
        "  X2[categoricals] = X2[categoricals].applymap(lambda x: x.lower())\n",
        "\n",
        "    # Replace common NAN values\n",
        "  nan_list = ['not known','unknown','none','-','##','not kno','unknown installer']\n",
        "  X2 = X2.replace(nan_list, np.nan)\n",
        "\n",
        "    # Any feature values with fewer than 100 rows gets turned into a NAN\n",
        "  for feature in X2[categoricals]:\n",
        "        # Determine which feature values to keep\n",
        "      to_keep = X2[feature].value_counts()[X2[feature].value_counts() > 100].index.tolist()\n",
        "        # Turn those into NANs (using a copy, to prevent warnings)\n",
        "      feature_copy = X2[feature].copy()\n",
        "      feature_copy[~feature_copy.isin(to_keep)] = np.nan\n",
        "      X2[feature] = feature_copy\n",
        "\n",
        "    # Fix all NANs\n",
        "  X2[categoricals] = X2[categoricals].fillna('other')\n",
        "    \n",
        "    \n",
        "  return X2\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "def cleanup4(X):\n",
        "    X2 = X.copy()\n",
        "    \n",
        "    garbage = ['longitude','latitude','construction_year_trash',\n",
        "              'latitude_trash','gps_height_trash',\n",
        "               'extraction_type_group','extraction_type_class',\n",
        "               'region_code','waterpoint_type_group','source_type',\n",
        "              'payment_type','quality_group','quantity_group']\n",
        "    \n",
        "    X2 = X2.drop(columns=garbage)\n",
        "    \n",
        "    X2['age'] = X2['date_recorded'] - X2['construction_year']\n",
        "\n",
        "    numericals = ['amount_tsh',\n",
        "                    'date_recorded',\n",
        "                    'gps_height',\n",
        "                    'num_private',\n",
        "                    'population',\n",
        "                    'construction_year',\n",
        "                    'age']\n",
        "\n",
        "    scaler = RobustScaler()\n",
        "    nums_scaled = scaler.fit_transform(X2[numericals])\n",
        "    nums_scaled = pd.DataFrame(nums_scaled, columns=numericals)\n",
        "    X2[numericals] = nums_scaled\n",
        "    \n",
        "    return X2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKGpCGF0kS5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nan_values_list = ['Not Known', 'Unknown', 'None', 'Not known', 'not known',\n",
        "                  '-', 'unknown', 'Unknown Installer', '##', 'none', '0']\n",
        "\n",
        "df = df.replace(nan_values_list, np.nan)\n",
        "testdf = testdf.replace(nan_values_list, np.nan)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfYK9G-FkWGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "red_list = ['Government', 'Central government', 'Central Govt', 'GOVER', 'Gover',\n",
        "            'Gove', 'Centr', 'Central Government', 'Wizara ya maji', 'Water', \n",
        "            'GOVERNMENT', 'Tanzania Government', 'GOVERN', 'central government',\n",
        "           'ISF/Government', 'CENTRAL GOVERNMENT', 'MINISTRY OF WATER', \n",
        "            'Ministry of water engineer', 'Cental Government', 'ADRA/Government']\n",
        "\n",
        "list2 = ['World vision', 'World Vision', 'WORLD VISION', 'World Vission', \n",
        "        'World Division', 'Government /World Vision', 'World visiin']\n",
        "\n",
        "\n",
        "df2['installer'] = df2['installer'].replace(red_list, 'government')\n",
        "df2['installer'] = df2['installer'].replace(list2, 'World Vision')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8-TtTxMkfBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*secondary.*$)', 'school')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*school.*$)', 'school')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*shule.*$)', 'school')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*sekondari.*$)', 'school')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*msingi.*$)', 'school')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*primary.*$)', 'school')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*clinic.*$)', 'health')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*hospital.*$)', 'health')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*zahanati.*$)', 'health')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*health.*$)', 'health')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*ccm.*$)', 'official')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*office.*$)', 'official')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*kijiji.*$)', 'official')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*ofis.*$)', 'official')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*idara.*$)', 'offical')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*maziwa.*$)', 'farm')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*farm.*$)', 'farm')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*maji.*$)', 'pump')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*water.*$)', 'pump')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*pump house.*$)', 'pump')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*pump.*$)', 'pump')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*bombani.*$)', 'pump')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*center.*$)', 'center')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*madukani.*$)', 'center')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*sokoni.*$)', 'center')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*market.*$)', 'center')\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*kwa.*$)', 'name')\n",
        "value_counts = df2['wpt_name'].value_counts()\n",
        "to_remove = value_counts[value_counts <= 300].index\n",
        "df2['wpt_name'].replace(to_remove, 'other', inplace=True)\n",
        "df2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*none.*$)', 'other')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}